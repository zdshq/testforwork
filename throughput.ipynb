{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "\n",
    "# 1. 创建 ViT-Tiny 模型，并加载 ImageNet 预训练权重\n",
    "model = timm.create_model('vit_tiny_patch16_224.augreg_in21k', pretrained=False, checkpoint_path=\"./vit_tiny_patch16_224.augreg_in21k.pth\") \n",
    "\n",
    "# (可选) 将模型移动到 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dense_part = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************moe part**************************************************\n",
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.slots\n",
      "blocks.6.mlp.experts.fc1.weight\n",
      "blocks.6.mlp.experts.fc1.bias\n",
      "blocks.6.mlp.experts.fc2.weight\n",
      "blocks.6.mlp.experts.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.slots\n",
      "blocks.7.mlp.experts.fc1.weight\n",
      "blocks.7.mlp.experts.fc1.bias\n",
      "blocks.7.mlp.experts.fc2.weight\n",
      "blocks.7.mlp.experts.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.slots\n",
      "blocks.8.mlp.experts.fc1.weight\n",
      "blocks.8.mlp.experts.fc1.bias\n",
      "blocks.8.mlp.experts.fc2.weight\n",
      "blocks.8.mlp.experts.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.slots\n",
      "blocks.9.mlp.experts.fc1.weight\n",
      "blocks.9.mlp.experts.fc1.bias\n",
      "blocks.9.mlp.experts.fc2.weight\n",
      "blocks.9.mlp.experts.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.slots\n",
      "blocks.10.mlp.experts.fc1.weight\n",
      "blocks.10.mlp.experts.fc1.bias\n",
      "blocks.10.mlp.experts.fc2.weight\n",
      "blocks.10.mlp.experts.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.slots\n",
      "blocks.11.mlp.experts.fc1.weight\n",
      "blocks.11.mlp.experts.fc1.bias\n",
      "blocks.11.mlp.experts.fc2.weight\n",
      "blocks.11.mlp.experts.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "head.weight\n",
      "head.bias\n",
      "**************************************************dense part**************************************************\n",
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "from model.vision_transformer_zeke import VisionTransformer_zeke\n",
    "\n",
    "moe_model = VisionTransformer_zeke(embed_dim=192, num_heads=3, num_classes=100)\n",
    "\n",
    "moe_part = moe_model.state_dict()\n",
    "\n",
    "\n",
    "moe_set = set()\n",
    "\n",
    "dense_set = set()\n",
    "\n",
    "print(\"*\"*50 + \"moe part\" + \"*\"*50)\n",
    "for key in moe_part.keys():\n",
    "    moe_set.add(key)\n",
    "    print(key)\n",
    "\n",
    "print(\"*\"*50 + \"dense part\" + \"*\"*50)\n",
    "for key in dense_part.keys():\n",
    "    dense_set.add(key)\n",
    "    print(key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************same part**************************************************\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.norm1.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.11.norm2.weight\n",
      "blocks.8.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.2.norm2.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.6.norm2.bias\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.4.norm2.bias\n",
      "blocks.3.mlp.fc1.bias\n",
      "cls_token\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.1.mlp.fc1.weight\n",
      "norm.bias\n",
      "norm.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.5.norm2.weight\n",
      "blocks.9.norm1.weight\n",
      "blocks.3.mlp.fc1.weight\n",
      "head.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.3.norm2.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.0.norm2.bias\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.7.norm1.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.6.norm1.bias\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.2.norm2.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.0.attn.proj.weight\n",
      "head.weight\n",
      "blocks.11.norm1.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.2.norm1.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.2.norm1.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.0.norm1.bias\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.4.attn.proj.bias\n",
      "**************************************************different part moe**************************************************\n",
      "blocks.9.mlp.experts.fc1.weight\n",
      "blocks.6.mlp.experts.fc1.bias\n",
      "blocks.7.mlp.experts.fc1.weight\n",
      "blocks.9.mlp.experts.fc2.bias\n",
      "blocks.6.mlp.experts.fc1.weight\n",
      "blocks.9.mlp.slots\n",
      "blocks.8.mlp.experts.fc2.bias\n",
      "blocks.8.mlp.slots\n",
      "blocks.7.mlp.experts.fc1.bias\n",
      "blocks.11.mlp.slots\n",
      "blocks.6.mlp.experts.fc2.bias\n",
      "blocks.11.mlp.experts.fc2.weight\n",
      "blocks.7.mlp.experts.fc2.weight\n",
      "blocks.10.mlp.experts.fc2.bias\n",
      "blocks.11.mlp.experts.fc1.weight\n",
      "blocks.7.mlp.experts.fc2.bias\n",
      "blocks.8.mlp.experts.fc1.weight\n",
      "blocks.8.mlp.experts.fc1.bias\n",
      "blocks.9.mlp.experts.fc1.bias\n",
      "blocks.10.mlp.slots\n",
      "blocks.10.mlp.experts.fc2.weight\n",
      "blocks.6.mlp.experts.fc2.weight\n",
      "blocks.11.mlp.experts.fc1.bias\n",
      "blocks.8.mlp.experts.fc2.weight\n",
      "blocks.10.mlp.experts.fc1.bias\n",
      "blocks.7.mlp.slots\n",
      "blocks.11.mlp.experts.fc2.bias\n",
      "blocks.6.mlp.slots\n",
      "blocks.10.mlp.experts.fc1.weight\n",
      "blocks.9.mlp.experts.fc2.weight\n",
      "**************************************************different part dense**************************************************\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.11.mlp.fc2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.8.mlp.fc1.bias\n"
     ]
    }
   ],
   "source": [
    "same_part = moe_set & dense_set\n",
    "\n",
    "diff_part_moe = moe_set - dense_set\n",
    "\n",
    "diff_part_dense = dense_set - moe_set\n",
    "\n",
    "print(\"*\"*50 + \"same part\" + \"*\"*50)\n",
    "for key in same_part:\n",
    "    print(key)\n",
    "\n",
    "print(\"*\"*50 + \"different part moe\" + \"*\"*50)\n",
    "for key in diff_part_moe:\n",
    "    print(key)\n",
    "\n",
    "print(\"*\"*50 + \"different part dense\" + \"*\"*50)\n",
    "for key in diff_part_dense:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "权重加载成功!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32010/1124953008.py:24: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647380992/work/aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  value[:] = dense_value.T\n"
     ]
    }
   ],
   "source": [
    "from typing import OrderedDict, Set\n",
    "from __future__ import annotations  # 添加这一行\n",
    "\n",
    "def get_extrator(moe_part: dict[str, torch.Tensor], dense_part: dict[str, torch.Tensor], same_part: set[str]) -> dict[str, torch.Tensor]:  \n",
    "    # 创建一个新的OrderedDict来存储修改后的权重\n",
    "    final_dict = OrderedDict()\n",
    "    \n",
    "    # 复制所有MOE模型的权重\n",
    "    for key, value in moe_part.items():\n",
    "        if key in same_part and \"head\" not in key:\n",
    "            # 对于相同部分（非head层），使用dense模型的权重\n",
    "            dense_value = dense_part[key]\n",
    "            if value.shape != dense_value.shape:\n",
    "                print(f\"skip {key}, shape error\")\n",
    "                final_dict[key] = value  # 保留原始权重\n",
    "            else:\n",
    "                final_dict[key] = dense_value  # 使用dense模型的权重\n",
    "        elif \"experts.\" in key:\n",
    "\n",
    "            temp = key.split(\".\")\n",
    "            del temp[3]\n",
    "            dense_key = \".\".join(temp)\n",
    "            dense_value = dense_part[dense_key]\n",
    "            value[:] = dense_value.T\n",
    "            final_dict[key] = value\n",
    "\n",
    "        else:\n",
    "            # 对于不同部分，保留MOE模型的原始权重\n",
    "            final_dict[key] = value\n",
    "    \n",
    "    return final_dict\n",
    "\n",
    "# 创建最终的权重字典\n",
    "final_dict = get_extrator(moe_part, dense_part, same_part)\n",
    "\n",
    "# 保存权重到文件\n",
    "torch.save(final_dict, \"moe_model_with_pretrained_weights.pth\")\n",
    "\n",
    "# 如果需要验证权重是否可加载，可以尝试加载\n",
    "# 测试加载权重到模型\n",
    "test_model = VisionTransformer_zeke(embed_dim=192, num_heads=3, num_classes=100)\n",
    "test_model.load_state_dict(final_dict)\n",
    "print(\"权重加载成功!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
